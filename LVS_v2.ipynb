{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysubparser import parser\n",
    "from pysubparser.cleaners import brackets, lower_case, formatting\n",
    "import datetime\n",
    "\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import RegexpParser\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module <tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x000002020824DB50> loaded\n"
     ]
    }
   ],
   "source": [
    "model = tf.saved_model.load(\"../USE_model/\")\n",
    "print(\"module %s loaded\" % model)\n",
    "def embed(input):\n",
    "    return model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "NP = r\"\"\"\n",
    "    NBAR:\n",
    "        {<NN.*|JJ>*<NN.*>}  # Nouns and Adjectives, terminated with Nouns\n",
    "    NP:\n",
    "        {<NBAR>}\n",
    "        {<NBAR><IN><NBAR>}  # Above, connected with in/of/etc...\n",
    "\"\"\"\n",
    "NP_chunker = RegexpParser(NP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sub_file(path):\n",
    "    subtitles = parser.parse(path)\n",
    "    subtitles = brackets.clean(\n",
    "        lower_case.clean(\n",
    "            formatting.clean(\n",
    "                subtitles\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return subtitles\n",
    "\n",
    "\n",
    "def read_gt_file(path):\n",
    "    gt_file = open(path)\n",
    "    file_content = gt_file.read().split(\"\\n\")\n",
    "    gt_file.close()\n",
    "    return file_content\n",
    "\n",
    "\n",
    "def get_gt_start_times(file_content):\n",
    "    start_times = []\n",
    "    for line in file_content:\n",
    "        st_time = line.split(\"\\t\")\n",
    "        if (len(st_time) > 1):\n",
    "            start_times.append(datetime.datetime.strptime(\n",
    "                st_time[1], \"%H:%M:%S,%f\"))\n",
    "    return start_times\n",
    "\n",
    "\n",
    "def prepare_text(subs):\n",
    "    sentences = []\n",
    "    start_times = []\n",
    "    for s in subs:\n",
    "        sentences.append(s.text)\n",
    "        start_times.append(s.start)\n",
    "\n",
    "    sentences = [word_tokenize(sent) for sent in sentences]\n",
    "    sentences = [pos_tag(w) for w in sentences]\n",
    "    sentences = [NP_chunker.parse(sent) for sent in sentences]\n",
    "\n",
    "    return sentences, start_times\n",
    "\n",
    "\n",
    "# takes the chunked data and extracts the noun phrases\n",
    "def parsed_text_to_noun_phrases(sentences):\n",
    "    nps = []\n",
    "    for sent in sentences:\n",
    "        tree = NP_chunker.parse(sent)\n",
    "        for subtree in tree.subtrees():\n",
    "            if subtree.label() == \"NP\":\n",
    "                t = subtree\n",
    "                t = ' '.join(word for word, tag in t.leaves())\n",
    "                nps.append(t)\n",
    "    return nps\n",
    "\n",
    "\n",
    "# builds the sliding window of given size and also returns the\n",
    "# init times of all the windows\n",
    "def build_sliding_window(init_times, sequence, window_size, step_size):\n",
    "    numOfChunks = ((len(sequence) - window_size) // step_size) + 1\n",
    "    print(\"\\nLength of sequence: \", len(sequence))\n",
    "    print(\"Number of chunks: \", numOfChunks)\n",
    "    req_chunks = []\n",
    "    req_window_start_times = []\n",
    "\n",
    "    for ith_chunk in range(0, numOfChunks*step_size, step_size):\n",
    "        req_chunks.append(sequence[ith_chunk: ith_chunk + window_size])\n",
    "        if (ith_chunk < len(init_times)):\n",
    "            req_window_start_times.append(init_times[ith_chunk])\n",
    "        else:\n",
    "            req_window_start_times.append(init_times[len(init_times) - 1])\n",
    "\n",
    "    return req_chunks, req_window_start_times\n",
    "\n",
    "\n",
    "def normalise_chunks(sentence_chunks):\n",
    "    sentences = []\n",
    "    for chunk in sentence_chunks:\n",
    "        sent = \" \".join(chunk)\n",
    "        sentences.append(sent)\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def plot_fig(x, score, heading, fig_no, can_plot):\n",
    "    if can_plot:\n",
    "        fig = plt.figure(fig_no, figsize=(10, 6))\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.plot(x, score, label=heading)\n",
    "\n",
    "    score_arr = np.array(score)\n",
    "    x_arr = np.array(x)\n",
    "\n",
    "    b = (np.diff(np.sign(np.diff(score_arr)))\n",
    "         > 0).nonzero()[0] + 1  # Local minimas\n",
    "\n",
    "    if can_plot:\n",
    "        ax.plot(x_arr[b], score_arr[b], \"o\",\n",
    "                label=\"Deepest Valleys\", color='r')\n",
    "        ax.legend()\n",
    "\n",
    "    return b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of noun phrases: 2826\n",
      "---------------------------------------------------------\n",
      "\n",
      "Window size:  60 Step size:  5 K:  30\n",
      "\n",
      "Length of sequence:  2826\n",
      "Number of chunks:  554\n",
      "[  1   4   6   8  13  16  20  23  26  28  32  35  38  44  48  52  56  58\n",
      "  60  64  67  70  72  75  79  81  85  87  91  93  97  99 103 106 108 110\n",
      " 115 118 122 124 126 128 131 134 136 139 142 145 149 151 154 157 159 162\n",
      " 164 167 169 172 176 178 181 186 190 193 198 201 205 208 210 213 215 218\n",
      " 220 225 228 230 232 234 236 241 246 248 252 256 258 260 263 269 272 275\n",
      " 277 279 286 289 293 296 301 306 310 312 316 319 322 326 328 332 334 336\n",
      " 340 344 349 351 353 356 359 362 364 366 369 371 373 376 378 382 386 388\n",
      " 391 393 396 400 403 405 407 412 415 417 419 424 427 430 434 437 439 442\n",
      " 452 455 457 464 468 470 474 477 479 483 486 488 492 495 497 500 502 504\n",
      " 506 509 514 518 520 522 525 527 529 532 534 537 539 541 544 547 549]\n",
      "no of boundaries 179\n",
      "Number of segments predicted: 22\n",
      "\n",
      "Precision: 0.12290502793296089 Recall: 1.1 F-Score: 0.22110552763819094\n",
      "---------------------------------------------------------\n",
      "---------------------------------------------------------\n",
      "\n",
      "Window size:  120 Step size:  10 K:  30\n",
      "\n",
      "Length of sequence:  2826\n",
      "Number of chunks:  271\n",
      "[  3   5   8  10  13  17  19  23  26  28  31  33  37  41  45  48  52  57\n",
      "  61  65  67  71  73  77  79  82  87  90  92  94  99 102 106 108 111 115\n",
      " 120 122 125 127 129 132 134 137 140 142 144 146 150 152 154 157 159 162\n",
      " 164 166 169 173 178 180 184 186 189 192 195 197 200 204 206 209 212 216\n",
      " 220 223 229 232 235 238 242 245 247 251 254 256 258 264 267]\n",
      "no of boundaries 87\n",
      "Number of segments predicted: 14\n",
      "\n",
      "Precision: 0.16091954022988506 Recall: 0.7 F-Score: 0.2616822429906542\n",
      "---------------------------------------------------------\n",
      "---------------------------------------------------------\n",
      "\n",
      "Window size:  180 Step size:  15 K:  30\n",
      "\n",
      "Length of sequence:  2826\n",
      "Number of chunks:  177\n",
      "[  2   5   9  12  15  17  19  21  23  28  30  32  34  39  42  47  49  51\n",
      "  53  57  59  62  66  68  70  74  76  79  82  84  87  91  94  96 101 103\n",
      " 106 108 114 116 118 121 124 133 135 140 142 144 148 152 154 157 160 163\n",
      " 169 172]\n",
      "no of boundaries 56\n",
      "Number of segments predicted: 8\n",
      "\n",
      "Precision: 0.14285714285714285 Recall: 0.4 F-Score: 0.21052631578947364\n",
      "---------------------------------------------------------\n",
      "---------------------------------------------------------\n",
      "\n",
      "Window size:  240 Step size:  20 K:  30\n",
      "\n",
      "Length of sequence:  2826\n",
      "Number of chunks:  130\n",
      "[  5   8  12  15  20  24  27  32  36  38  40  42  47  49  52  54  56  59\n",
      "  64  68  72  79  83  90  94  99 102 104 107 111 113 117 120 123 125]\n",
      "no of boundaries 35\n",
      "Number of segments predicted: 8\n",
      "\n",
      "Precision: 0.22857142857142856 Recall: 0.4 F-Score: 0.2909090909090909\n",
      "---------------------------------------------------------\n",
      "---------------------------------------------------------\n",
      "\n",
      "Window size:  300 Step size:  25 K:  30\n",
      "\n",
      "Length of sequence:  2826\n",
      "Number of chunks:  102\n",
      "[ 1  4  7  9 14 16 19 25 30 35 37 40 43 45 48 50 53 56 65 67 70 73 77 80\n",
      " 84 86 88 91 95 97]\n",
      "no of boundaries 30\n",
      "Number of segments predicted: 3\n",
      "\n",
      "Precision: 0.1 Recall: 0.15 F-Score: 0.12\n",
      "---------------------------------------------------------\n",
      "---------------------------------------------------------\n",
      "\n",
      "Window size:  360 Step size:  30 K:  30\n",
      "\n",
      "Length of sequence:  2826\n",
      "Number of chunks:  83\n",
      "[ 1  3  8 12 15 18 20 24 29 31 34 38 40 42 47 52 55 57 62 66 69 74 76 79]\n",
      "no of boundaries 24\n",
      "Number of segments predicted: 9\n",
      "\n",
      "Precision: 0.375 Recall: 0.45 F-Score: 0.4090909090909091\n",
      "---------------------------------------------------------\n",
      "---------------------------------------------------------\n",
      "\n",
      "Window size:  420 Step size:  35 K:  30\n",
      "\n",
      "Length of sequence:  2826\n",
      "Number of chunks:  69\n",
      "[ 1  4  6  8 11 13 15 18 20 22 24 29 31 33 36 42 46 48 52 55 58 60 64]\n",
      "no of boundaries 23\n",
      "Number of segments predicted: 4\n",
      "\n",
      "Precision: 0.17391304347826086 Recall: 0.2 F-Score: 0.18604651162790697\n",
      "---------------------------------------------------------\n",
      "---------------------------------------------------------\n",
      "\n",
      "Window size:  480 Step size:  40 K:  30\n",
      "\n",
      "Length of sequence:  2826\n",
      "Number of chunks:  59\n",
      "[ 2  4  6  8 11 14 18 20 24 27 29 32 36 40 42 44 47 51 54]\n",
      "no of boundaries 19\n",
      "Number of segments predicted: 5\n",
      "\n",
      "Precision: 0.2631578947368421 Recall: 0.25 F-Score: 0.25641025641025644\n",
      "---------------------------------------------------------\n",
      "---------------------------------------------------------\n",
      "\n",
      "Window size:  540 Step size:  45 K:  30\n",
      "\n",
      "Length of sequence:  2826\n",
      "Number of chunks:  51\n",
      "[ 2  5  7 10 12 14 16 21 25 28 30 32 34 36 38 41 43 45]\n",
      "no of boundaries 18\n",
      "Number of segments predicted: 5\n",
      "\n",
      "Precision: 0.2777777777777778 Recall: 0.25 F-Score: 0.2631578947368421\n",
      "---------------------------------------------------------\n",
      "---------------------------------------------------------\n",
      "\n",
      "Window size:  600 Step size:  50 K:  30\n",
      "\n",
      "Length of sequence:  2826\n",
      "Number of chunks:  45\n",
      "[ 3  6  9 13 19 21 25 28 31 33 36 38 41]\n",
      "no of boundaries 13\n",
      "Number of segments predicted: 3\n",
      "\n",
      "Precision: 0.23076923076923078 Recall: 0.15 F-Score: 0.18181818181818185\n",
      "---------------------------------------------------------\n",
      "---------------------------------------------------------\n",
      "\n",
      "Window size:  660 Step size:  55 K:  30\n",
      "\n",
      "Length of sequence:  2826\n",
      "Number of chunks:  40\n",
      "[ 3  6 11 13 17 22 25 27 29 32 36]\n",
      "no of boundaries 11\n",
      "Number of segments predicted: 6\n",
      "\n",
      "Precision: 0.5454545454545454 Recall: 0.3 F-Score: 0.3870967741935483\n",
      "---------------------------------------------------------\n",
      "---------------------------------------------------------\n",
      "\n",
      "Window size:  720 Step size:  60 K:  30\n",
      "\n",
      "Length of sequence:  2826\n",
      "Number of chunks:  36\n",
      "[ 1  5  7  9 15 19 22 26 30]\n",
      "no of boundaries 9\n",
      "Number of segments predicted: 4\n",
      "\n",
      "Precision: 0.4444444444444444 Recall: 0.2 F-Score: 0.2758620689655173\n",
      "---------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def vectorise(sentences, sentence_len, fig_no):\n",
    "    sentence_embeddings = embed(sentences)\n",
    "    score = []\n",
    "    for i in range(0, len(sentence_embeddings) - 2):\n",
    "        x = tf.nn.l2_normalize(sentence_embeddings[i:i+1], axis=1)\n",
    "        y = tf.nn.l2_normalize(sentence_embeddings[i+1:i+2], axis=1)\n",
    "        cos_sim = tf.reduce_sum(tf.multiply(x, y), axis=1)\n",
    "        score.append(cos_sim[0])\n",
    "    # Plotting Cosine Similarity\n",
    "    minimas = plot_fig(range(1, len(sentence_embeddings) - 1), score,\n",
    "                       'Cosine Similarity with window size ' + str(sentence_len), fig_no, False)\n",
    "    return score, minimas\n",
    "\n",
    "\n",
    "def get_k_best_points(sentences, local_minimas, win_start_times, k_val):\n",
    "    selected_windows = []\n",
    "    counter = 0\n",
    "    for m in local_minimas:\n",
    "#         if (counter == k_val):\n",
    "#             break\n",
    "        selected_windows.append((sentences[m], win_start_times[m]))\n",
    "#         counter += 1\n",
    "\n",
    "    return selected_windows\n",
    "\n",
    "\n",
    "subtitles = read_sub_file(\n",
    "    \"../Lecture_Video_Fragmentation_Dataset/ALV_srt/0025.srt\"\n",
    ")\n",
    "gt_content = read_gt_file(\n",
    "    \"../Lecture_Video_Fragmentation_Dataset/ALV_srt_GT/0025.txt\"\n",
    ")\n",
    "gt_start_times = get_gt_start_times(gt_content)\n",
    "cleaned_text, init_times = prepare_text(subtitles)\n",
    "noun_phrases = parsed_text_to_noun_phrases(cleaned_text)\n",
    "\n",
    "print(\"Number of noun phrases: \" + str(len(noun_phrases)))\n",
    "\n",
    "window_sizes = [60, 120, 180, 240, 300, 360, 420, 480, 540, 600, 660, 720]\n",
    "# window_sizes = [600, 660, 720, 780, 840, 900, 960, 1020, 1080, 1140, 1200]\n",
    "\n",
    "for window_size in window_sizes:\n",
    "    step_size = window_size // 12\n",
    "    k = 30\n",
    "    print(\"---------------------------------------------------------\")\n",
    "    print(\"\\nWindow size: \", window_size,\n",
    "          \"Step size: \", step_size, \"K: \", k)\n",
    "\n",
    "    chunks, window_start_times = build_sliding_window(\n",
    "        init_times, noun_phrases, window_size, step_size)\n",
    "    messages = normalise_chunks(chunks)\n",
    "    cos_sim_scores, loc_minimas  = vectorise(messages, window_size, 1)\n",
    "    print(loc_minimas)\n",
    "    boundaries = get_k_best_points(messages, loc_minimas, window_start_times, k)\n",
    "\n",
    "    print(\"no of boundaries \" + str(len(boundaries)))\n",
    "    arr = sorted(boundaries, key=lambda x: x[1])\n",
    "    pos_predictions = 1\n",
    "    \n",
    "    for i in range(1, len(gt_start_times)):\n",
    "        for j in range(0, len(arr)):\n",
    "            dt = datetime.datetime.combine(\n",
    "                datetime.date(1900, 1, 1), arr[j][1])\n",
    "            diff = gt_start_times[i] - dt\n",
    "            if (abs(diff.total_seconds()) < 30):\n",
    "                pos_predictions += 1\n",
    "#                 print(\"\\n\")\n",
    "#                 print(\"boundary: \" + str(dt))\n",
    "#                 print(\"actual start time: \" + str(gt_start_times[i]))\n",
    "#                 print(\"difference in seconds: \" + str(abs(diff.total_seconds())))\n",
    "#                 print(\"\\n\")\n",
    "\n",
    "    print(\"Number of segments predicted: \" + str(pos_predictions))\n",
    "    p = pos_predictions / len(boundaries)\n",
    "    r = pos_predictions / len(gt_start_times)\n",
    "    f = (2 * p * r) / (p + r)\n",
    "    print(\"\\nPrecision: \" + str(p), \"Recall: \" + str(r), \"F-Score: \" + str(f))\n",
    "    print(\"---------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
